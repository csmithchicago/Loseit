{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Cleaning r/loseit Challenge Data\n",
    "\n",
    "This is done using the reddit API praw and the Google Sheets API gspread. Add more info ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import praw\n",
    "import datetime as dt\n",
    "import markdown\n",
    "from lxml import etree\n",
    "import lxml\n",
    "import gspread\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function used to sort some of the lists in human ordered form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/raw_data/'):\n",
    "    os.makedirs('./data/raw_data/')\n",
    "if not os.path.exists('./figures/'):\n",
    "    os.makedirs('./figures/')\n",
    "if not os.path.exists('./data/cleaned/'):\n",
    "    os.makedirs('./data/cleaned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit('loseit_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loseit_sub = reddit.subreddit('loseit')\n",
    "challenge_posts = loseit_sub.search(\"loseit challenge tracker\", limit=1000)\n",
    "topics_dict = { \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [],\n",
    "                \"created\": [], \"body\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in challenge_posts:\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    topics_dict[\"body\"].append(submission.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data.to_csv('loseit_search_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have searched through old loseit posts, we need to find the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t\t"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for body in topics_dict['body']:\n",
    "    try:\n",
    "        doc = etree.fromstring(markdown.markdown(re.sub('[\\\\n]', '', body)))\n",
    "        for link in doc.xpath('//a'):\n",
    "            web_url = link.get('href')\n",
    "            if bool(re.search('spreadsheet',web_url)):\n",
    "                links.append(web_url)\n",
    "    except etree.XMLSyntaxError:\n",
    "        print('',end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_spreadsheets = list(set(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the Google API to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sheet name: rebirth_challenge_2017, link: https://docs.google.com/spreadsheets/d/1BAsiKlOCty_2sZeb93ogQ40tNJSIgREfar5NcnFGU4E/edit?usp=sharing\n",
      "\tsheet name: spring_into_summer_challenge, link: https://docs.google.com/spreadsheets/d/11-26-l8mHoBq-XMUTtq7AlXfIWvPllBSL1r1hbq1xnA/edit#gid=1429543432\n",
      "sheet name: spring_time_to_energize_challenge, link: https://docs.google.com/spreadsheets/d/1R62EK0PZeQJL0Cfd1UyMThaAU8IDZgD_f7R9aqonGVU/edit#gid=1340513170\n",
      "\tsheet name: new_year_new_you_2016_challenge_master_spreadsheet, link: https://docs.google.com/spreadsheets/d/1PQpubmKrJcrA_bEBJZD_YUo87WYkxiu09CFK-ppehlQ/edit#gid=1711453384\n",
      "\tsheet name: lord_of_the_rings_loseit_summer_challenge_sign_ups, link: https://docs.google.com/spreadsheets/d/1VIRc3Xu7havYhzHWYIyt_gWacTUP3T33NGCIG-lXBfw/edit?usp=sharing\n",
      "\tsheet name: tracker_loseit_spring_semester_challenge, link: https://docs.google.com/spreadsheet/ccc?key=0Ajn9WUix2Lq1dFpHWG14N0hOV1Q2SzUxRXlBNE9JUEE&usp=drive_web#gid=0\n",
      "\tsheet name: super_hero_summer_challenge_'17_, link: https://docs.google.com/spreadsheets/d/1VMfjVfSFs2wFLf3j_zUwABJJIZTyIdfDFraECHZnYyA/edit?usp=sharing\n",
      "\tsheet name: scifi_movies_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1R8S2AFe8VctpjyxXSCuB7ngOfzUx-2uQwYZEdn1Hd3o/edit?usp=sharing\n",
      "\tsheet name: autumn_animal_challenge, link: https://docs.google.com/spreadsheets/d/1VVy-qPRnhI2syUC4-oo-OyD7DlK4LmZ3P6nIxhzbDgo/edit#gid=1072324023\n",
      "\tsheet name: the_summer_challenge_2016, link: https://docs.google.com/spreadsheets/d/1qcOU8dp6Nz-mzkZZlr-9RJgo7hXF_DT0vuFykwkaek4/edit?usp=sharing\n",
      "sheet name: super_mario_brothers_super_loseit_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1cEHfD_pQZw5a48fw8wUHg6QwFEPVzwF8vmgGE5KVj2Y/edit#gid=1034497164\n",
      "\tsheet name: summer_'14_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1eNKmSi64PvPNw0iA3mOoFh2bkiJ6utyMtP95qmny9qE/edit?pli=1#gid=0\n",
      "\tsheet name: loseit_2018_mythical_creatures_challenge_spring_edition, link: https://docs.google.com/spreadsheets/d/1RdZFKErF7ppL-a7VhcJkpz6yoU5RMimXfmntn4GS5ZY/edit#gid=745292442\n",
      "\tsheet name: new_challenge_new_year_new_goals_2018_edition, link: https://docs.google.com/spreadsheets/d/1AJWpv86_vzngxqTJ8EO9_8MyfqJNLYGjSbkq6mljfIQ/edit?usp=sharing\n",
      "\t"
     ]
    }
   ],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "names = []\n",
    "for spreadsheet_link in unique_spreadsheets:\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('loseit-sheets-6012c29a1f40.json', scope)\n",
    "    gc = gspread.authorize(creds)\n",
    "    sht = gc.open_by_url(spreadsheet_link)\n",
    "    if bool(re.search('nter', sht.title)) == False and bool(re.search('/r/', sht.title)) == False and bool(re.search('Calculator', sht.title)) == False:\n",
    "        sheet_name = re.sub('_\\(responses\\)' ,'', re.sub(',', '', re.sub('\\]','', re.sub('\\[','', re.sub(' ', '_', re.sub('  ', '_', re.sub('-', '', sht.title.lower())))))))\n",
    "        if sheet_name not in names:\n",
    "            print(f'sheet name: {sheet_name}, link: {spreadsheet_link}')\n",
    "            names.append(sheet_name)\n",
    "            try: \n",
    "                data_sheet = sht.worksheet('Tracker')\n",
    "                data_vals = data_sheet.get_all_values()\n",
    "                data_df = pd.DataFrame(data_vals[1:-2], columns=data_vals[0])\n",
    "                data_df.to_csv('./data/raw_data/' + sheet_name + '.csv')\n",
    "            except gspread.WorksheetNotFound:\n",
    "                try:\n",
    "                    data_sheet = sht.worksheet('Master Spreadsheet')\n",
    "                    data_vals = data_sheet.get_all_values()\n",
    "                    data_df = pd.DataFrame(data_vals[1:-2], columns=data_vals[0])\n",
    "                    data_df.to_csv('./data/raw_data/' + sheet_name + '.csv')\n",
    "                except gspread.WorksheetNotFound:\n",
    "                    print('',end='\\t')#sheet_name)\n",
    "            else:\n",
    "                print('',end='\\t')#sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is cleaning up some of the column information, and removing the information that is not useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_col = {f'W{x}':f'Week {x}' for x in range(0, 11)}\n",
    "new_names = {'W0 (SW)': 'Week 0', 'Sex': 'Gender', 'Male, Female, Other': 'Gender',\n",
    "             'TEAM': 'Team', 'Teams': 'Team', 'Challenge GW': 'Challenge Goal Weight',\n",
    "             'Challenge SW': 'Week 0', 'MyFitnessPal Username/Link': 'MFP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "p = Path('./data/raw_data/')\n",
    "for challenge in p.rglob('*.csv'):\n",
    "    # Read in the csv files and change some of the column names\n",
    "    test_df = pd.read_csv(challenge, index_col=0)\n",
    "    test_df.dropna(axis=1, how='all')\n",
    "    test_df.columns = test_df.columns.str.strip().str.replace('?', '').str.replace(':','')\n",
    "    test_df.rename(columns=new_names, inplace=True)\n",
    "    \n",
    "    # timestamp\n",
    "    if 'Timestamp' not in test_df:\n",
    "        test_df['Timestamp'] = np.NaN\n",
    "    # Age\n",
    "    test_df['Age'] = test_df[test_df.filter(regex=re.compile('Age', re.IGNORECASE)).columns[0]]\n",
    "    \n",
    "    # Gender\n",
    "    if len(test_df.filter(regex=re.compile('Sex', re.IGNORECASE)).columns):\n",
    "        test_df['Gender'] = test_df[test_df.filter(regex=re.compile('Sex', re.IGNORECASE)).columns[0]] \n",
    "    if len(test_df.filter(regex=re.compile('Gender', re.IGNORECASE)).columns):\n",
    "        test_df['Gender'] = test_df[test_df.filter(regex=re.compile('Gender', re.IGNORECASE)).columns[0]] \n",
    "    if 'Gender' not in test_df:\n",
    "        test_df['Gender'] = 'Unknown'\n",
    "\n",
    "    # Ignore KGS\n",
    "    if len(test_df.filter(regex=re.compile('kgs', re.IGNORECASE)).columns):\n",
    "        test_df.drop(test_df.filter(regex=re.compile('kgs', re.IGNORECASE)).columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    # Keep Just Starting BMI\n",
    "    test_df.drop(test_df.filter(regex=re.compile('BMI', re.IGNORECASE)).columns[1:], axis=1, inplace=True)\n",
    "    \n",
    "    # Username\n",
    "    test_df.columns = test_df.columns.str.replace(test_df.filter(like='name').columns[0], 'Username')\n",
    "    \n",
    "    # Weigh-in Data\n",
    "    test_df.rename(columns=weeks_col, inplace=True)            \n",
    "    if len(test_df.filter(regex=re.compile(\"week 0\", re.IGNORECASE)).columns):\n",
    "        test_df['Week 0'] = test_df[test_df.filter(regex=re.compile(\"week 0\", re.IGNORECASE)).columns[0]]\n",
    "    elif len(test_df.filter(regex=re.compile(\"sign-up\", re.IGNORECASE)).columns):\n",
    "        test_df['Week 0'] = test_df[test_df.filter(regex=re.compile(\"sign-up\", re.IGNORECASE)).columns[0]]\n",
    "    elif len(test_df.filter(regex=re.compile(\"start weight\", re.IGNORECASE)).columns):\n",
    "        test_df['Week 0'] = test_df[test_df.filter(regex=re.compile(\"start weight\", re.IGNORECASE)).columns[0]]\n",
    "    elif len(test_df.filter(regex=re.compile(\"Signup weight\", re.IGNORECASE)).columns):\n",
    "        test_df['Week 0'] = test_df[test_df.filter(regex=re.compile(\"Signup weight\", re.IGNORECASE)).columns[0]]\n",
    "    elif len(test_df.filter(regex=re.compile('What is your current weight', re.IGNORECASE)).columns):\n",
    "        test_df['Week 0'] = test_df[test_df.filter(regex=re.compile('What is your current weight', re.IGNORECASE)).columns[0]]\n",
    "\n",
    "    # Height\n",
    "    test_df['Height'] = test_df[test_df.filter(regex=re.compile(\"Height\", re.IGNORECASE)).columns[0]]   \n",
    "\n",
    "    # Highest Weight\n",
    "    if len(test_df.filter(regex=re.compile('Highest', re.IGNORECASE)).columns):\n",
    "        test_df['Highest Weight'] = test_df[test_df.filter(regex=re.compile('Highest', re.IGNORECASE)).columns[0]]\n",
    "    else:\n",
    "        test_df['Highest Weight'] = np.NaN\n",
    "    \n",
    "    # Has NSV\n",
    "    test_df['Has NSV'] = test_df[test_df.filter(regex=re.compile(\"NSV\", re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    test_df['NSV Text'] = test_df[test_df.filter(regex=re.compile(\"NSV\", re.IGNORECASE)).columns[0]].astype(str).replace('nan', '')\n",
    "    \n",
    "    # Goal Weight\n",
    "    test_df['Challenge Goal Weight'] = test_df[test_df.filter(regex=re.compile(\"Goal Weight\", re.IGNORECASE)).columns[0]]\n",
    "\n",
    "    # Has a food tracker\n",
    "    if len(test_df.filter(regex=re.compile('MyFitnessPal', re.IGNORECASE)).columns):\n",
    "        test_df['MFP'] = test_df[test_df.filter(regex=re.compile('MyFitnessPal', re.IGNORECASE)).columns[0]].notnull().astype('int')    \n",
    "    test_df['Has MFP'] = test_df[test_df.filter(regex=re.compile(\"MFP\", re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    if len(test_df.filter(regex=re.compile(\"Loseit\", re.IGNORECASE)).columns):\n",
    "        test_df['Has Loseit'] = test_df[test_df.filter(regex=re.compile(\"Loseit\", re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    else:\n",
    "        test_df['Has Loseit'] = 0\n",
    "    test_df['Has Food Tracker'] = test_df['Has MFP'] + test_df['Has Loseit']\n",
    "    test_df['Has Food Tracker'] = test_df['Has Food Tracker'].replace(2,1)\n",
    "    \n",
    "    # fitness tracker\n",
    "    if len(test_df.filter(regex=re.compile('Fitbit', re.IGNORECASE)).columns):\n",
    "        test_df['Has Activity Tracker'] = test_df[test_df.filter(regex=re.compile('Fitbit', re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    elif len(test_df.filter(regex=re.compile('Fitness tracker', re.IGNORECASE)).columns):\n",
    "        test_df['Has Activity Tracker'] = test_df[test_df.filter(regex=re.compile('Fitness Tracker', re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    elif len(test_df.filter(regex=re.compile('Garmin', re.IGNORECASE)).columns):\n",
    "        test_df['Has Activity Tracker'] = test_df[test_df.filter(regex=re.compile('Garmin', re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    "    elif len(test_df.filter(regex=re.compile('Strava', re.IGNORECASE)).columns):\n",
    "        test_df['Has Activity Tracker'] = test_df[test_df.filter(regex=re.compile('Strava', re.IGNORECASE)).columns[0]].notnull().astype('int')\n",
    " \n",
    "    # Starting Weight\n",
    "    test_df['Starting Weight'] = test_df['Week 0']\n",
    "        \n",
    "    # Create the final Data Frame\n",
    "    col_weeks = test_df.filter(regex=re.compile('Week', re.IGNORECASE)).columns.tolist()\n",
    "    col_weeks.sort(key=natural_keys)\n",
    "    col_names = ['Timestamp', 'Username', 'Team', 'Age', 'Gender', 'Height','Highest Weight', \n",
    "                 'Starting Weight', 'Challenge Goal Weight', 'Starting BMI', 'Has NSV', \n",
    "                 'Has Food Tracker', 'Has Activity Tracker', 'NSV Text']\n",
    "    \n",
    "    data_cols = col_names + list(col_weeks)\n",
    "    data_df = test_df[data_cols]\n",
    "\n",
    "    df_list.append((challenge.stem, data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data contains only what we are interested in learning, we need to fill in any missing values before we combine all of the challenges together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df_list = []\n",
    "\n",
    "for data in df_list:\n",
    "    df = data[1].copy()\n",
    "\n",
    "    # Some odties in the data\n",
    "    if data[0] == 'spring_time_to_energize_challenge':\n",
    "        df.drop([448, 828], inplace=True)\n",
    "        df.replace({'ERROR': np.NaN}, inplace=True)\n",
    "    if data[0] == 'autumn_animal_challenge':\n",
    "        df.drop(971, inplace=True)\n",
    "        df.replace({'#DIV/0!': np.NaN, 'old':np.NaN}, inplace=True)\n",
    "    if data[0] == 'rebirth_challenge_2017':\n",
    "        df.drop(['Week 7', 'Week 8'], axis=1, inplace=True)\n",
    "        df.replace({'20s': 25, 'Yes': np.NaN}, inplace=True)\n",
    "\n",
    "\n",
    "    df.dropna(subset=['Username', 'Challenge Goal Weight'], axis=0, inplace=True)\n",
    "    df.loc[pd.isnull(df['Gender']), 'Gender'] = 'Unknown'\n",
    "    df.loc[~df['Gender'].isin(['Female', 'Male', 'Unknown']), 'Gender'] = 'Other'\n",
    "    df.loc[pd.isnull(df['Highest Weight']), 'Highest Weight'] = df['Week 0']\n",
    "    df['Timestamp'] = df['Timestamp'].fillna(axis=0, method='ffill', limit=10)\n",
    "\n",
    "    '''\n",
    "    Now we want to convert the series into the correct types\n",
    "    '''\n",
    "    numberic = ['Age', 'Height','Highest Weight', 'Starting Weight', 'Challenge Goal Weight', 'Starting BMI']\n",
    "    df[numberic] = df[numberic].astype(np.float64)\n",
    "    \n",
    "    '''\n",
    "    Now we need to work on removing those who dropped out of the challenge. \n",
    "    First, if only one weigh-in was missed we will fill it with the previous weeks \n",
    "    weigh-in. Next, we remove any that are missing the final weigh-in, and lastly, \n",
    "    we fill any of the remaining missing values with the previous weeks data.\n",
    "    '''\n",
    "    \n",
    "    weight_cols = df.columns.values[14:].tolist()\n",
    "\n",
    "    df[weight_cols] = df[weight_cols].fillna(axis=1, method='ffill', limit=1)\n",
    "    df.dropna(axis=0, subset=[weight_cols[-1]], inplace=True)\n",
    "    df[weight_cols] = df[weight_cols].fillna(axis=1, method='ffill').astype(np.float64)\n",
    "\n",
    "    '''\n",
    "    Now we want to create a couple more columns for total weight lost, \n",
    "    percentage lost, percentage of challenge weight lost\n",
    "    '''\n",
    "    \n",
    "    new_cols = ['Final Weight', 'Total Challenge Loss', 'Challenge Percentage Lost', 'Percent of Challenge Goal']\n",
    "    \n",
    "    df['Challenge Goal Loss'] = df['Starting Weight'].astype(np.float64) - df['Challenge Goal Weight'].astype(np.float64)\n",
    "    df[new_cols[0]] = df[weight_cols[-1]]\n",
    "    df[new_cols[1]] = df[weight_cols[0]] - df[weight_cols[-1]]\n",
    "    df[new_cols[2]] = (df[new_cols[1]] / df[weight_cols[0]]) * 100\n",
    "    df[new_cols[3]] = (df[new_cols[1]] / (df['Starting Weight'].astype(np.float64) - df['Challenge Goal Weight'].astype(np.float64))).replace(np.inf,0).replace(-np.inf,0) * 100\n",
    "    df[new_cols] = df[new_cols].astype(np.float64)\n",
    "    \n",
    "    df = df[df.columns.values[:14].tolist() + ['Challenge Goal Loss'] + new_cols]\n",
    "\n",
    "    # Save the cleaned data and append to the dataframe list\n",
    "    data_df.to_csv('./data/cleaned/cleaned_' + data[0] + '.csv')\n",
    "    \n",
    "    big_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat(big_df_list, ignore_index=True).dropna()\n",
    "big_df.to_csv('./data/cleaned_and_combined_loseit_challenge_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data saved and cleaned, we can move onto [Analyze Challenge Data](analyze_loseit_challenge_data.ipynb) to analyze the data and maybe make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1022\n",
      "986\n",
      "860\n",
      "889\n",
      "752\n",
      "860\n",
      "800\n",
      "744\n",
      "1068\n",
      "682\n"
     ]
    }
   ],
   "source": [
    "for ex in big_df_list:\n",
    "    print(len(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
