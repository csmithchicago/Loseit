{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding and Cleaning r/loseit Challenge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gspread'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-62d8e448e43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgspread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmarkdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gspread'"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gspread\n",
    "import lxml\n",
    "import markdown\n",
    "import pandas as pd\n",
    "import praw\n",
    "import seaborn as sns\n",
    "from lxml import etree\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function used to sort some of the lists in human ordered form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    \"\"\"\n",
    "    return [atoi(c) for c in re.split(\"(\\d+)\", text)]\n",
    "\n",
    "\n",
    "def month_year(timestamps):\n",
    "    return str(timestamps.date())[:-3]\n",
    "\n",
    "\n",
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data Directories\n",
    "if not os.path.exists(\"./data/raw_data/\"):\n",
    "    os.makedirs(\"./data/raw_data/\")\n",
    "if not os.path.exists(\"./figures/\"):\n",
    "    os.makedirs(\"./figures/\")\n",
    "if not os.path.exists(\"./data/cleaned_data/\"):\n",
    "    os.makedirs(\"./data/cleaned_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loseit_data is the name at the top of the praw.ini file\n",
    "reddit = praw.Reddit(\"loseit_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loseit_sub = reddit.subreddit(\"loseit\")\n",
    "challenge_posts = loseit_sub.search(\"loseit challenge tracker\", limit=1000)\n",
    "topics_dict = {\n",
    "    \"title\": [],\n",
    "    \"score\": [],\n",
    "    \"id\": [],\n",
    "    \"url\": [],\n",
    "    \"comms_num\": [],\n",
    "    \"created\": [],\n",
    "    \"body\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in challenge_posts:\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    topics_dict[\"body\"].append(submission.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data = pd.DataFrame(topics_dict)\n",
    "\n",
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "topics_data = topics_data.assign(timestamp=_timestamp)\n",
    "\n",
    "topics_data.to_csv(\"../data/raw_data/loseit_search_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t\t"
     ]
    }
   ],
   "source": [
    "# Now that we have searched through old loseit posts, we need to find the urls.\n",
    "links = []\n",
    "for body in topics_dict[\"body\"]:\n",
    "    try:\n",
    "        doc = etree.fromstring(markdown.markdown(re.sub(\"[\\\\n]\", \"\", body)))\n",
    "        for link in doc.xpath(\"//a\"):\n",
    "            web_url = link.get(\"href\")\n",
    "            if bool(re.search(\"spreadsheet\", web_url)) and bool(\n",
    "                re.search(\"oogle\", web_url)\n",
    "            ):\n",
    "                links.append(web_url)\n",
    "    except etree.XMLSyntaxError:\n",
    "        pass\n",
    "\n",
    "unique_spreadsheets = list(set(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sheet name: summer_'14_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1eNKmSi64PvPNw0iA3mOoFh2bkiJ6utyMtP95qmny9qE/edit?pli=1#gid=0\n",
      "\tsheet name: spring_time_to_energize_challenge, link: https://docs.google.com/spreadsheets/d/1R62EK0PZeQJL0Cfd1UyMThaAU8IDZgD_f7R9aqonGVU/edit?usp=sharing\n",
      "\tsheet name: super_mario_brothers_super_loseit_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1cEHfD_pQZw5a48fw8wUHg6QwFEPVzwF8vmgGE5KVj2Y/edit#gid=1034497164\n",
      "\tsheet name: tracker_loseit_spring_semester_challenge, link: https://docs.google.com/spreadsheet/ccc?key=0Ajn9WUix2Lq1dFpHWG14N0hOV1Q2SzUxRXlBNE9JUEE#gid=0\n",
      "\tsheet name: the_summer_challenge_2016, link: https://docs.google.com/spreadsheets/d/1qcOU8dp6Nz-mzkZZlr-9RJgo7hXF_DT0vuFykwkaek4/edit?usp=sharing\n",
      "sheet name: new_year_new_you_2016_challenge_master_spreadsheet, link: https://docs.google.com/spreadsheets/d/1PQpubmKrJcrA_bEBJZD_YUo87WYkxiu09CFK-ppehlQ/edit?usp=sharing\n",
      "\tsheet name: lord_of_the_rings_loseit_summer_challenge_sign_ups, link: https://docs.google.com/spreadsheets/d/1VIRc3Xu7havYhzHWYIyt_gWacTUP3T33NGCIG-lXBfw/edit#gid=837248805\n",
      "\tsheet name: spring_into_summer_challenge, link: https://docs.google.com/spreadsheets/d/11-26-l8mHoBq-XMUTtq7AlXfIWvPllBSL1r1hbq1xnA/edit#gid=1429543432\n",
      "sheet name: autumn_animal_challenge, link: https://docs.google.com/spreadsheets/d/1VVy-qPRnhI2syUC4-oo-OyD7DlK4LmZ3P6nIxhzbDgo/edit?usp=sharing\n",
      "\tsheet name: rebirth_challenge_2017, link: https://docs.google.com/spreadsheets/d/1BAsiKlOCty_2sZeb93ogQ40tNJSIgREfar5NcnFGU4E/edit?usp=sharing\n",
      "\tsheet name: scifi_movies_challenge_tracker, link: https://docs.google.com/spreadsheets/d/1R8S2AFe8VctpjyxXSCuB7ngOfzUx-2uQwYZEdn1Hd3o/edit?usp=sharing\n",
      "\tsheet name: loseit_2018_mythical_creatures_challenge_spring_edition, link: https://docs.google.com/spreadsheets/d/1RdZFKErF7ppL-a7VhcJkpz6yoU5RMimXfmntn4GS5ZY/edit?usp=sharing\n",
      "\tsheet name: new_challenge_new_year_new_goals_2018_edition, link: https://docs.google.com/spreadsheets/d/1AJWpv86_vzngxqTJ8EO9_8MyfqJNLYGjSbkq6mljfIQ/edit?usp=sharing\n",
      "\tsheet name: super_hero_summer_challenge_'17_, link: https://docs.google.com/spreadsheets/d/1VMfjVfSFs2wFLf3j_zUwABJJIZTyIdfDFraECHZnYyA/edit?usp=sharing\n",
      "\t"
     ]
    }
   ],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "names = []\n",
    "for spreadsheet_link in unique_spreadsheets:\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        \"loseit-sheets-6012c29a1f40.json\", scope  # this is the google-app.json file\n",
    "    )\n",
    "    gc = gspread.authorize(creds)\n",
    "    sht = gc.open_by_url(spreadsheet_link)\n",
    "    if (\n",
    "        bool(re.search(\"nter\", sht.title)) == False\n",
    "        and bool(re.search(\"/r/\", sht.title)) == False\n",
    "        and bool(re.search(\"Calculator\", sht.title)) == False\n",
    "    ):\n",
    "        sheet_name = re.sub(\n",
    "            \"_\\(responses\\)\",\n",
    "            \"\",\n",
    "            re.sub(\n",
    "                \",\",\n",
    "                \"\",\n",
    "                re.sub(\n",
    "                    \"\\]\",\n",
    "                    \"\",\n",
    "                    re.sub(\n",
    "                        \"\\[\",\n",
    "                        \"\",\n",
    "                        re.sub(\n",
    "                            \" \",\n",
    "                            \"_\",\n",
    "                            re.sub(\"  \", \"_\", re.sub(\"-\", \"\", sht.title.lower())),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        if sheet_name not in names:\n",
    "            print(f\"sheet name: {sheet_name}, link: {spreadsheet_link}\")\n",
    "            names.append(sheet_name)\n",
    "            try:\n",
    "                data_sheet = sht.worksheet(\"Tracker\")\n",
    "                data_vals = data_sheet.get_all_values()\n",
    "                data_df = pd.DataFrame(data_vals[1:-2], columns=data_vals[0])\n",
    "                data_df.to_csv(\"../data/raw_data/\" + sheet_name + \".csv\")\n",
    "            except gspread.WorksheetNotFound:\n",
    "                try:\n",
    "                    data_sheet = sht.worksheet(\"Master Spreadsheet\")\n",
    "                    data_vals = data_sheet.get_all_values()\n",
    "                    data_df = pd.DataFrame(data_vals[1:-2], columns=data_vals[0])\n",
    "                    data_df.to_csv(\"../data/raw_data/\" + sheet_name + \".csv\")\n",
    "                except gspread.WorksheetNotFound:\n",
    "                    print(\"\", end=\"\\t\")  # sheet_name)\n",
    "            else:\n",
    "                print(\"\", end=\"\\t\")  # sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is cleaning up some of the column information, and removing the information that is not useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_col = {f\"W{x}\": f\"Week {x}\" for x in range(0, 11)}\n",
    "new_names = {\n",
    "    \"W0 (SW)\": \"Week 0\",\n",
    "    \"Sex\": \"Gender\",\n",
    "    \"Male, Female, Other\": \"Gender\",\n",
    "    \"TEAM\": \"Team\",\n",
    "    \"Teams\": \"Team\",\n",
    "    \"Challenge GW\": \"Challenge Goal Weight\",\n",
    "    \"Challenge SW\": \"Week 0\",\n",
    "    \"MyFitnessPal Username/Link\": \"MFP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "p = Path(\"../data/raw_data/\")\n",
    "for idx, challenge in enumerate(p.rglob(\"*.csv\")):\n",
    "    # Challenge Names\n",
    "    challenge_name = re.sub(\"\\d\", \"\", challenge.name[:-4])\n",
    "\n",
    "    # Read in the csv files and change some of the column names\n",
    "    test_df = pd.read_csv(challenge, index_col=0)\n",
    "    test_df.dropna(axis=1, how=\"all\")\n",
    "    test_df.columns = (\n",
    "        test_df.columns.str.strip().str.replace(\"?\", \"\").str.replace(\":\", \"\")\n",
    "    )\n",
    "    test_df.rename(columns=new_names, inplace=True)\n",
    "\n",
    "    # timestamp\n",
    "    if \"Timestamp\" not in test_df:\n",
    "        test_df[\"Timestamp\"] = (\n",
    "            \"October 2018\"\n",
    "            if challenge_name == \"super_mario_brothers_super_loseit_challenge_tracker\"\n",
    "            else \"March 2017\"\n",
    "        )\n",
    "    test_df.Timestamp = pd.to_datetime(test_df.Timestamp, errors=\"coerce\").apply(\n",
    "        month_year\n",
    "    )\n",
    "\n",
    "    # Age\n",
    "    test_df[\"Age\"] = test_df[\n",
    "        test_df.filter(regex=re.compile(\"Age\", re.IGNORECASE)).columns[0]\n",
    "    ]\n",
    "\n",
    "    # Gender\n",
    "    if len(test_df.filter(regex=re.compile(\"Sex\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Gender\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"Sex\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    if len(test_df.filter(regex=re.compile(\"Gender\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Gender\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"Gender\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    if \"Gender\" not in test_df:\n",
    "        test_df[\"Gender\"] = \"Unknown\"\n",
    "\n",
    "    # Ignore KGS\n",
    "    if len(test_df.filter(regex=re.compile(\"kgs\", re.IGNORECASE)).columns):\n",
    "        test_df.drop(\n",
    "            test_df.filter(regex=re.compile(\"kgs\", re.IGNORECASE)).columns[0],\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    # Keep Just Starting BMI\n",
    "    test_df.drop(\n",
    "        test_df.filter(regex=re.compile(\"BMI\", re.IGNORECASE)).columns[1:],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Username\n",
    "    test_df.columns = test_df.columns.str.replace(\n",
    "        test_df.filter(like=\"name\").columns[0], \"Username\"\n",
    "    )\n",
    "    test_df.Username = test_df.Username.astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "    # Weigh-in Data\n",
    "    test_df.rename(columns=weeks_col, inplace=True)\n",
    "    if len(test_df.filter(regex=re.compile(\"week 0\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Week 0\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"week 0\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    elif len(test_df.filter(regex=re.compile(\"sign-up\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Week 0\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"sign-up\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    elif len(test_df.filter(regex=re.compile(\"start weight\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Week 0\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"start weight\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    elif len(test_df.filter(regex=re.compile(\"Signup weight\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Week 0\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"Signup weight\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    elif len(\n",
    "        test_df.filter(\n",
    "            regex=re.compile(\"What is your current weight\", re.IGNORECASE)\n",
    "        ).columns\n",
    "    ):\n",
    "        test_df[\"Week 0\"] = test_df[\n",
    "            test_df.filter(\n",
    "                regex=re.compile(\"What is your current weight\", re.IGNORECASE)\n",
    "            ).columns[0]\n",
    "        ]\n",
    "\n",
    "    # Height\n",
    "    test_df[\"Height\"] = test_df[\n",
    "        test_df.filter(regex=re.compile(\"Height\", re.IGNORECASE)).columns[0]\n",
    "    ]\n",
    "\n",
    "    # Highest Weight\n",
    "    if len(test_df.filter(regex=re.compile(\"Highest\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Highest Weight\"] = test_df[\n",
    "            test_df.filter(regex=re.compile(\"Highest\", re.IGNORECASE)).columns[0]\n",
    "        ]\n",
    "    else:\n",
    "        test_df[\"Highest Weight\"] = np.NaN\n",
    "\n",
    "    # Has NSV\n",
    "    test_df[\"Has NSV\"] = (\n",
    "        test_df[test_df.filter(regex=re.compile(\"NSV\", re.IGNORECASE)).columns[0]]\n",
    "        .notnull()\n",
    "        .astype(\"int\")\n",
    "    )\n",
    "    test_df[\"NSV Text\"] = (\n",
    "        test_df[test_df.filter(regex=re.compile(\"NSV\", re.IGNORECASE)).columns[0]]\n",
    "        .astype(str)\n",
    "        .replace(\"nan\", \"\")\n",
    "    )\n",
    "\n",
    "    # Goal Weight\n",
    "    test_df[\"Challenge Goal Weight\"] = test_df[\n",
    "        test_df.filter(regex=re.compile(\"Goal Weight\", re.IGNORECASE)).columns[0]\n",
    "    ]\n",
    "\n",
    "    # Has a food tracker\n",
    "    if len(test_df.filter(regex=re.compile(\"MyFitnessPal\", re.IGNORECASE)).columns):\n",
    "        test_df[\"MFP\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(regex=re.compile(\"MyFitnessPal\", re.IGNORECASE)).columns[\n",
    "                    0\n",
    "                ]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "    test_df[\"Has MFP\"] = (\n",
    "        test_df[test_df.filter(regex=re.compile(\"MFP\", re.IGNORECASE)).columns[0]]\n",
    "        .notnull()\n",
    "        .astype(\"int\")\n",
    "    )\n",
    "    if len(test_df.filter(regex=re.compile(\"Loseit\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Has Loseit\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(regex=re.compile(\"Loseit\", re.IGNORECASE)).columns[0]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "    else:\n",
    "        test_df[\"Has Loseit\"] = 0\n",
    "    test_df[\"Has Food Tracker\"] = test_df[\"Has MFP\"] + test_df[\"Has Loseit\"]\n",
    "    test_df[\"Has Food Tracker\"] = test_df[\"Has Food Tracker\"].replace(2, 1)\n",
    "\n",
    "    # fitness tracker\n",
    "    if len(test_df.filter(regex=re.compile(\"Fitbit\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Has Activity Tracker\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(regex=re.compile(\"Fitbit\", re.IGNORECASE)).columns[0]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "    elif len(\n",
    "        test_df.filter(regex=re.compile(\"Fitness tracker\", re.IGNORECASE)).columns\n",
    "    ):\n",
    "        test_df[\"Has Activity Tracker\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(\n",
    "                    regex=re.compile(\"Fitness Tracker\", re.IGNORECASE)\n",
    "                ).columns[0]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "    elif len(test_df.filter(regex=re.compile(\"Garmin\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Has Activity Tracker\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(regex=re.compile(\"Garmin\", re.IGNORECASE)).columns[0]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "    elif len(test_df.filter(regex=re.compile(\"Strava\", re.IGNORECASE)).columns):\n",
    "        test_df[\"Has Activity Tracker\"] = (\n",
    "            test_df[\n",
    "                test_df.filter(regex=re.compile(\"Strava\", re.IGNORECASE)).columns[0]\n",
    "            ]\n",
    "            .notnull()\n",
    "            .astype(\"int\")\n",
    "        )\n",
    "\n",
    "    # Team and Challenge Names\n",
    "    test_df[\"Challenge\"] = (\n",
    "        challenge_name.replace(\"_\", \" \")\n",
    "        .title()\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"Tracker\", \"\")\n",
    "        .replace(\"Master\", \"\")\n",
    "        .replace(\"Sign Ups\", \"\")\n",
    "        .replace(\"Spreadsheet\", \"\")\n",
    "        .replace(\"Loseit\", \"\")\n",
    "        .replace(\"Challenge\", \"\")\n",
    "        .replace(\"Edition\", \"\")\n",
    "        .replace(\"  \", \" \")\n",
    "        .strip()\n",
    "        + \" Challenge\"\n",
    "    )\n",
    "    test_df[\"Team\"] = test_df[\"Team\"].str.title()\n",
    "    test_df[\"Team\"] = test_df[\"Team\"].str.replace(\"2Nd\", \"2nd\")\n",
    "\n",
    "    # Starting Weight\n",
    "    test_df[\"Starting Weight\"] = test_df[\"Week 0\"]\n",
    "\n",
    "    # Create the final Data Frame\n",
    "    col_weeks = test_df.filter(regex=re.compile(\"Week\", re.IGNORECASE)).columns.tolist()\n",
    "    col_weeks.sort(key=natural_keys)\n",
    "    col_names = [\n",
    "        \"Timestamp\",\n",
    "        \"Username\",\n",
    "        \"Team\",\n",
    "        \"Challenge\",\n",
    "        \"Age\",\n",
    "        \"Gender\",\n",
    "        \"Height\",\n",
    "        \"Highest Weight\",\n",
    "        \"Starting Weight\",\n",
    "        \"Challenge Goal Weight\",\n",
    "        \"Starting BMI\",\n",
    "        \"Has NSV\",\n",
    "        \"Has Food Tracker\",\n",
    "        \"Has Activity Tracker\",\n",
    "        \"NSV Text\",\n",
    "    ]\n",
    "\n",
    "    data_cols = col_names + list(col_weeks)\n",
    "    data_df = test_df[data_cols]\n",
    "\n",
    "    df_list.append((challenge.stem, data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data contains only what we are interested in learning, we need to fill in any missing values before we combine all of the challenges together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df_list = []\n",
    "\n",
    "for data in df_list:\n",
    "    df = data[1].copy()\n",
    "\n",
    "    # Some odties in the data\n",
    "    if data[0] == \"spring_time_to_energize_challenge\":\n",
    "        df.drop([448, 828], inplace=True)\n",
    "        df.replace({\"ERROR\": np.NaN}, inplace=True)\n",
    "    if data[0] == \"autumn_animal_challenge\":\n",
    "        df.drop(971, inplace=True)\n",
    "        df.replace({\"#DIV/0!\": np.NaN, \"old\": np.NaN}, inplace=True)\n",
    "    if data[0] == \"rebirth_challenge_2017\":\n",
    "        df.drop([\"Week 7\", \"Week 8\"], axis=1, inplace=True)\n",
    "        df.replace({\"20s\": 25, \"Yes\": np.NaN}, inplace=True)\n",
    "\n",
    "    df.Timestamp = statistics.mode(df.Timestamp)\n",
    "\n",
    "    df.dropna(subset=[\"Username\", \"Challenge Goal Weight\"], axis=0, inplace=True)\n",
    "    df.loc[pd.isnull(df[\"Gender\"]), \"Gender\"] = \"Unknown\"\n",
    "    df.loc[~df[\"Gender\"].isin([\"Female\", \"Male\", \"Unknown\"]), \"Gender\"] = \"Other\"\n",
    "    df.loc[pd.isnull(df[\"Highest Weight\"]), \"Highest Weight\"] = df[\"Week 0\"]\n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].fillna(axis=0, method=\"ffill\", limit=10)\n",
    "\n",
    "    \n",
    "    # Now we want to convert the series into the correct types\n",
    "    numberic = [\n",
    "        \"Age\",\n",
    "        \"Height\",\n",
    "        \"Highest Weight\",\n",
    "        \"Starting Weight\",\n",
    "        \"Challenge Goal Weight\",\n",
    "        \"Starting BMI\",\n",
    "    ]\n",
    "    df[numberic] = df[numberic].astype(np.float64)\n",
    "\n",
    "    \"\"\"\n",
    "    Now we need to work on removing those who dropped out of the challenge. \n",
    "    First, if only one weigh-in was missed we will fill it with the previous weeks \n",
    "    weigh-in. Next, we remove any that are missing the final weigh-in, and lastly, \n",
    "    we fill any of the remaining missing values with the previous weeks data.\n",
    "    \"\"\"\n",
    "    weight_cols = df.columns.values[15:].tolist()\n",
    "\n",
    "    df[weight_cols] = df[weight_cols].fillna(axis=1, method=\"ffill\", limit=1)\n",
    "    df.dropna(axis=0, subset=[weight_cols[-1]], inplace=True)\n",
    "    df[weight_cols] = df[weight_cols].fillna(axis=1, method=\"ffill\").astype(np.float64)\n",
    "\n",
    "    new_cols = [\n",
    "        \"Final Weight\",\n",
    "        \"Total Challenge Loss\",\n",
    "        \"Challenge Percentage Lost\",\n",
    "        \"Percent of Challenge Goal\",\n",
    "    ]\n",
    "\n",
    "    df[\"Challenge Goal Loss\"] = df[\"Starting Weight\"].astype(np.float64) - df[\n",
    "        \"Challenge Goal Weight\"\n",
    "    ].astype(np.float64)\n",
    "    df[new_cols[0]] = df[weight_cols[-1]]\n",
    "    df[new_cols[1]] = df[weight_cols[0]] - df[weight_cols[-1]]\n",
    "    df[new_cols[2]] = (df[new_cols[1]] / df[weight_cols[0]]) * 100\n",
    "    df[new_cols[3]] = (\n",
    "        df[new_cols[1]]\n",
    "        / (\n",
    "            df[\"Starting Weight\"].astype(np.float64)\n",
    "            - df[\"Challenge Goal Weight\"].astype(np.float64)\n",
    "        )\n",
    "    ).replace(np.inf, 0).replace(-np.inf, 0) * 100\n",
    "    df[new_cols] = df[new_cols].astype(np.float64)\n",
    "\n",
    "    df = df[df.columns.values[:15].tolist() + [\"Challenge Goal Loss\"] + new_cols]\n",
    "\n",
    "    # Save the cleaned data and append to the dataframe list\n",
    "    df.to_csv(\"../data/cleaned_data/cleaned_\" + data[0] + \".csv\")\n",
    "\n",
    "    big_df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat(big_df_list, ignore_index=True).dropna()\n",
    "big_df.to_csv(\"../data/processed_data/cleaned_and_combined_loseit_challenge_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data saved and cleaned, we can move onto [Inspect Challenge Data](02_inspect_loseit_challenge_data.ipynb) to look a little bit deeper into the data to see if there are any outliers and how to possibly deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
